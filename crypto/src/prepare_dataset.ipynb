{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/data.parquet\")\n",
    "# events = pd.read_parquet(\"../data/events.parquet\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fib_retracement(data: pd.DataFrame, high: str, low: str, target_column: str, levels: list = [0.236, 0.382, 0.618, 1.0]):\n",
    "    for level in levels:\n",
    "        data[f\"{target_column}_fib_{level}\"] = data[high] - (data[high] - data[low]) * level\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_lag_features(data: pd.DataFrame, target_column, lag_steps=1):\n",
    "    if isinstance(lag_steps, int):\n",
    "        for i in range(1, lag_steps + 1):\n",
    "            data[f\"{target_column}_lag_{i}\"] = data[target_column].shift(i)\n",
    "\n",
    "    if isinstance(lag_steps, list):\n",
    "        for i in lag_steps:\n",
    "            data[f\"{target_column}_lag_{i}\"] = data[target_column].shift(i)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def simple_moving_average(data: pd.DataFrame, target_column: str | list, window_size: int | list = 3):\n",
    "    if isinstance(target_column, list) and isinstance(window_size, list):\n",
    "        for col in target_column:\n",
    "            for window in window_size:\n",
    "                data[f\"{col}_rolling_mean_{window}\"] = data[col].rolling(window=window).mean()\n",
    "\n",
    "    elif isinstance(target_column, str):\n",
    "        data[f\"{target_column}_rolling_mean_{window_size}\"] = data[target_column].rolling(window=window_size).mean()\n",
    "\n",
    "    elif isinstance(target_column, list):\n",
    "        for col in target_column:\n",
    "            data[f\"{col}_rolling_mean_{window_size}\"] = data[col].rolling(window=window_size).mean()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def exponential_moving_average(data: pd.DataFrame, target_column: str | list, window_size: int | list = 3):\n",
    "    if isinstance(target_column, str):\n",
    "        target_column = [target_column]\n",
    "    if isinstance(window_size, int):\n",
    "        window_size = [window_size]\n",
    "\n",
    "    for col in target_column:\n",
    "        for window in window_size:\n",
    "            data[f\"{col}_rolling_exp_mean_{window}\"] = data[col].ewm(span=window).mean()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def bollinger_bands(data: pd.DataFrame, target_column: str, window_size: int = 20):\n",
    "    data[f\"{target_column}_rolling_mean_{window_size}\"] = data[target_column].rolling(window=window_size).mean()\n",
    "    data[f\"{target_column}_rolling_std_{window_size}\"] = data[target_column].rolling(window=window_size).std()\n",
    "    data[f\"{target_column}_bollinger_upper_{window_size}\"] = data[f\"{target_column}_rolling_mean_{window_size}\"] + 2 * data[\n",
    "        f\"{target_column}_rolling_std_{window_size}\"]\n",
    "    data[f\"{target_column}_bollinger_lower_{window_size}\"] = data[f\"{target_column}_rolling_mean_{window_size}\"] - 2 * data[\n",
    "        f\"{target_column}_rolling_std_{window_size}\"]\n",
    "\n",
    "    return data\n",
    "\n",
    "def greater_than(data: pd.DataFrame, src_column: str, target_column: str):\n",
    "    \"\"\"\n",
    "    Check if the src_column is greater then target_column\n",
    "    Return 1 if src_column is greater then target_column and 0 otherwise\n",
    "    \"\"\"\n",
    "    data[f\"{src_column}_greater_than_{target_column}\"] = np.where(data[src_column] > data[target_column], 1, 0)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# def plot_bollinger_bands(data: pd.DataFrame, target_column: str, window_size: int = 20):\n",
    "#     data[f\"{target_column}_rolling_mean_{window_size}\"] = data[target_column].rolling(window=window_size).mean()\n",
    "#     data[f\"{target_column}_rolling_std_{window_size}\"] = data[target_column].rolling(window=window_size).std()\n",
    "#     data[f\"{target_column}_bollinger_upper_{window_size}\"] = data[f\"{target_column}_rolling_mean_{window_size}\"] + 2 * data[\n",
    "#         f\"{target_column}_rolling_std_{window_size}\"]\n",
    "#     data[f\"{target_column}_bollinger_lower_{window_size}\"] = data[f\"{target_column}_rolling_mean_{window_size}\"] - 2 * data[\n",
    "#         f\"{target_column}_rolling_std_{window_size}\"]\n",
    "\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(data[target_column], label=\"Close Price\", color=\"blue\")\n",
    "#     plt.plot(data[f\"{target_column}_rolling_mean_{window_size}\"], label=\"Rolling Mean\", color=\"red\")\n",
    "#     plt.plot(data[f\"{target_column}_bollinger_upper_{window_size}\"], label=\"Bollinger Upper\", color=\"green\")\n",
    "#     plt.plot(data[f\"{target_column}_bollinger_lower_{window_size}\"], label=\"Bollinger Lower\", color=\"green\")\n",
    "#     plt.title(f\"{target_column} Bollinger Bands\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "def average_true_range(data: pd.DataFrame, window_size: int = 14):\n",
    "    data[f\"high_low_{window_size}\"] = data[\"high\"] - data[\"low\"]\n",
    "    data[f\"high_close_{window_size}\"] = np.abs(data[\"high\"] - data[\"close\"].shift(1))\n",
    "    data[f\"low_close_{window_size}\"] = np.abs(data[\"low\"] - data[\"close\"].shift(1))\n",
    "    data[f\"true_range_{window_size}\"] = np.max(\n",
    "        [data[f\"high_low_{window_size}\"], data[f\"high_close_{window_size}\"], data[f\"low_close_{window_size}\"]], axis=0)\n",
    "    data[f\"average_true_range_{window_size}\"] = data[f\"true_range_{window_size}\"].rolling(window=window_size).mean()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def relative_strength_index(data: pd.DataFrame, column: str = \"close\", window_size: int = 14):\n",
    "    delta = data[column].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window_size).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window_size).mean()\n",
    "    rs = gain / loss\n",
    "    data[f\"rsi_{window_size}\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def macd(data: pd.DataFrame, column: str = \"close\", short_window: int = 12, long_window: int = 26, signal_window: int = 9):\n",
    "    data[f\"short_ema_{short_window}\"] = data[column].ewm(span=short_window, adjust=False).mean()\n",
    "    data[f\"long_ema_{long_window}\"] = data[column].ewm(span=long_window, adjust=False).mean()\n",
    "    data[f\"macd_{short_window}_{long_window}\"] = data[f\"short_ema_{short_window}\"] - data[f\"long_ema_{long_window}\"]\n",
    "    data[f\"signal_{signal_window}\"] = data[f\"macd_{short_window}_{\n",
    "        long_window}\"].ewm(span=signal_window, adjust=False).mean()\n",
    "    data[f\"macd_hist_{short_window}_{long_window}_{signal_window}\"] = data[f\"macd_{\n",
    "        short_window}_{long_window}\"] - data[f\"signal_{signal_window}\"]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def stochastic_oscillator(data: pd.DataFrame, window_size: int = 14):\n",
    "    data[f\"stochastic_oscillator_{window_size}\"] = (data[\"close\"] - data[\"low\"].rolling(window=window_size).min()) / (\n",
    "        data[\"high\"].rolling(window=window_size).max() - data[\"low\"].rolling(window=window_size).min())\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def williams_r(data: pd.DataFrame, window_size: int = 14):\n",
    "    data[f\"williams_r_{window_size}\"] = (data[\"high\"].rolling(window=window_size).max() - data[\"close\"]) / (\n",
    "        data[\"high\"].rolling(window=window_size).max() - data[\"low\"].rolling(window=window_size).min())\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def money_flow_index(data: pd.DataFrame, window_size: int = 14):\n",
    "    typical_price = (data[\"high\"] + data[\"low\"] + data[\"close\"]) / 3\n",
    "    raw_money_flow = typical_price * data[\"volume\"]\n",
    "    money_flow_ratio = raw_money_flow.rolling(window=window_size).sum() / (typical_price * data[\"volume\"]).rolling(\n",
    "        window=window_size).sum()\n",
    "    data[f\"money_flow_index_{window_size}\"] = 100 - (100 / (1 + money_flow_ratio))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def on_balance_volume(data: pd.DataFrame, offset: int = 1):\n",
    "    data[f\"on_balance_volume_{offset}\"] = np.where(\n",
    "        data[\"close\"] > data[\"close\"].shift(offset), data[\"volume\"], -data[\"volume\"])\n",
    "    data[f\"on_balance_volume_{offset}\"] = data[f\"on_balance_volume_{offset}\"].cumsum()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def volume_weighted_average_price(data: pd.DataFrame, window_size: int = 14):\n",
    "    data[f\"volume_weighted_average_price_{window_size}\"] = (\n",
    "        data[\"close\"] * data[\"volume\"]).rolling(window=window_size).sum() / data[\"volume\"].rolling(window=window_size).sum()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def volitility(data: pd.DataFrame, column: str = \"close\", window_size: int = 14):\n",
    "    data[f\"volitility_{column}_{window_size}\"] = data[column].rolling(window=window_size).std()\n",
    "    data[f\"volitility_pct_change_{window_size}_{column}\"] = data[column].pct_change().rolling(window=window_size).std()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def apply_technical_indicators(data: pd.DataFrame, target_column: str | list = \"close\", window_size: int | list = 14):\n",
    "\n",
    "    if isinstance(target_column, str):\n",
    "        target_column = [target_column]\n",
    "    if isinstance(window_size, int):\n",
    "        window_size = [window_size]\n",
    "\n",
    "    for col in target_column:\n",
    "        for window in window_size:\n",
    "            data = volitility(data, col, window)\n",
    "            data = average_true_range(data, window)\n",
    "            data = simple_moving_average(data, col, window)\n",
    "            data = exponential_moving_average(data, col, window)\n",
    "            data = relative_strength_index(data, col, window)\n",
    "            data = macd(data, col)\n",
    "            data = stochastic_oscillator(data, window)\n",
    "            data = williams_r(data, window)\n",
    "            data = money_flow_index(data, window)\n",
    "            data = on_balance_volume(data)\n",
    "            data = volume_weighted_average_price(data, window)\n",
    "            data = bollinger_bands(data, col, window)\n",
    "            # data = fib_retracement(data, \"high\", \"low\", col)\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def forward_fill_with_decay(df: pd.DataFrame, column, decay_factor):\n",
    "    \"\"\"\n",
    "    Forward fills missing values in a specified column with a decay factor applied.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame containing the data.\n",
    "    - column: The column name (or index) to apply forward fill with decay.\n",
    "    - decay_factor: The factor by which the previous value decays (0 < decay_factor < 1).\n",
    "\n",
    "    Returns:\n",
    "    - The DataFrame with missing values filled with decayed forward fill.\n",
    "    \"\"\"\n",
    "    if not 0 < decay_factor < 1:\n",
    "        raise ValueError(\"Decay factor must be between 0 and 1.\")\n",
    "\n",
    "    df = df.copy()  # Avoid modifying the original DataFrame\n",
    "    df[column] = df[column].astype(float)  # Ensure column is of float type for NaNs\n",
    "\n",
    "    # Initialize variables\n",
    "    last_value = np.nan\n",
    "    decay = 1.0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if pd.notna(df.at[i, column]):\n",
    "            # Update the last value and reset decay\n",
    "            last_value = df.at[i, column]\n",
    "            decay = 1.0\n",
    "            df.at[i, column] = last_value * decay\n",
    "            decay *= decay_factor  # Apply the decay factor for the next value\n",
    "\n",
    "        elif pd.isna(df.at[i, column]) and pd.notna(last_value):\n",
    "            # Apply decay to the last value and fill the current cell\n",
    "            df.at[i, column] = last_value * decay\n",
    "            decay *= decay_factor  # Apply the decay factor for the next value\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_event(data: pd.DataFrame, event: pd.DataFrame, prefix_name, falloff: bool = True, decay_factor: float = 0.99) -> pd.DataFrame:\n",
    "    \"\"\"Join Events df to data df based on Date\"\"\"\n",
    "    event = event.copy(deep=True)\n",
    "\n",
    "    event[\"date\"] = pd.to_datetime(event[\"date\"])\n",
    "\n",
    "    # Rename Events columns to event_* to avoid conflicts\n",
    "    for column in event.columns:\n",
    "        if column != \"date\":\n",
    "            event.rename(columns={column: f\"event_{column}\"}, inplace=True)\n",
    "\n",
    "    df = data.merge(event, on=\"date\", how=\"left\").copy(deep=True)\n",
    "\n",
    "    df[\"event_decay\"] = np.where(pd.notna(df[\"event_name\"]), 1, np.nan)\n",
    "    df[\"event_name\"] = df[\"event_name\"].ffill().infer_objects(copy=False)\n",
    "    df[\"event_value\"] = df[\"event_value\"].ffill().infer_objects(copy=False)\n",
    "    df[\"event_sentiment\"] = df[\"event_sentiment\"].ffill().infer_objects(copy=False)\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"event_name\": f\"{prefix_name}_event_name\",\n",
    "        \"event_value\": f\"{prefix_name}_event_value\",\n",
    "        \"event_sentiment\": f\"{prefix_name}_event_sentiment\",\n",
    "        \"event_decay\": f\"{prefix_name}_event_decay\"\n",
    "    })\n",
    "\n",
    "    # forward fill sentiment with decay\n",
    "    if falloff:\n",
    "        df = forward_fill_with_decay(df, f\"{prefix_name}_event_decay\", decay_factor)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def day_of_week(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data[\"day_of_week\"] = data[\"date\"].dt.dayofweek.astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "def day_of_month(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data[\"day_of_month\"] = data[\"date\"].dt.day.astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "def month(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data[\"month\"] = data[\"date\"].dt.month.astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "def year(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data[\"year\"] = data[\"date\"].dt.year.astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "def week_of_year(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data[\"week_of_year\"] = data[\"date\"].dt.isocalendar().week.astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "def is_not_nan_column_and_default(data: pd.DataFrame, column) -> pd.DataFrame:\n",
    "    if isinstance(column, list):\n",
    "        for col in column:\n",
    "            data[f\"is_nan_{col}\"] = data[col].notna().astype(int)\n",
    "            data[col] = data[col].fillna(0)\n",
    "        return data\n",
    "\n",
    "    if isinstance(column, str):\n",
    "        data[f\"is_nan_{column}\"] = data[column].notna().astype(int)\n",
    "        data[column] = data[column].fillna(0)\n",
    "        return data\n",
    "\n",
    "\n",
    "def future_value(data: pd.DataFrame, column: str, offset: int, drop_recent: bool = True) -> pd.DataFrame:\n",
    "    data[f\"target_{column}\"] = data[column].shift(-offset)\n",
    "    # Drop last X offset rows\n",
    "    if drop_recent:  # Drop to remove NaNs\n",
    "        data = drop_recent_with_offset(data, offset)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def drop_recent_with_offset(data: pd.DataFrame, offset: int) -> pd.DataFrame:\n",
    "    return data[:-offset]\n",
    "\n",
    "\n",
    "def percent_change_between_columns(data: pd.DataFrame, column: str, column2: str) -> pd.DataFrame:\n",
    "    data[f\"percent_change_{column}\"] = (data[column2] - data[column]) / data[column]\n",
    "    return data\n",
    "\n",
    "\n",
    "def percent_change(data: pd.DataFrame, column: str, offset: int | list = 1, drop_recent: bool = True) -> pd.DataFrame:\n",
    "    if isinstance(offset, int):\n",
    "        offset = [offset]\n",
    "\n",
    "    for off in offset:\n",
    "        data[f\"{column}_percent_change_{off}\"] = (data[column].shift(-off) - data[column]) / data[column]\n",
    "\n",
    "    if drop_recent:  # Drop to remove NaNs\n",
    "        data = drop_recent_with_offset(data, max(offset))\n",
    "\n",
    "    return data\n",
    "\n",
    "def up_down(data: pd.DataFrame, column: str, offset: int, drop_recent: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simply tells wheither the price went up or down in the last window_size days\n",
    "        Returns 1 if the price went up, 0 if the price went down, and 0 if the price stayed the same\n",
    "    \"\"\"\n",
    "    if isinstance(offset, int):\n",
    "        offset = [offset]\n",
    "\n",
    "    for off in offset:\n",
    "        data[f\"up_down_{column}_{off}\"] = np.where(data[column].shift(-off) > data[column], 1,\n",
    "                                                    np.where(data[column].shift(-off) < data[column], 0, 0))\n",
    "        \n",
    "    if drop_recent:  # Drop to remove NaNs\n",
    "        data = drop_recent_with_offset(data, max(offset))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_event_for_crossing(data: pd.DataFrame,  src_column: str, target_column: str, decay_factor: float = 0.98) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create an event for when the target_column crosses the src_column.\n",
    "    If the target_column crosses the src_column from below to above, the apply_event\n",
    "\n",
    "    create_event_for_crossing(df, \"close_bollinger_lower_30\", \"close\") will create an event for when the close price crosses the lower bollinger band\n",
    "\n",
    "    IF you want to create an event for when the close price crosses the upper bollinger band, you can use create_event_for_crossing(df, \"close\", \"close_bollinger_upper_30\")\n",
    "    \n",
    "    The src column should be the lower value and the target column should be the higher value\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a Dataframe of the events. The Event schema needs to be in this format [date, name, value, sentiment]\n",
    "    events = pd.DataFrame(columns=[\"date\", \"name\", \"value\", \"sentiment\"])\n",
    "    event_name = f\"crossed_{src_column}_{target_column}\"\n",
    "\n",
    "    # Loop through the data and check for the crossing. If the target_column crosses the src_column from below to above, add an event\n",
    "    for i in range(1, len(data)):\n",
    "        if data[src_column][i] > data[target_column][i] and data[src_column][i-1] < data[target_column][i-1]:\n",
    "            \n",
    "            temp_df = pd.DataFrame([[data[\"date\"][i], \"crossed\", 1, 1]], columns=events.columns)\n",
    "            events = pd.concat([events, temp_df], axis=0)\n",
    "\n",
    "    data = apply_event(data, events, event_name, decay_factor=decay_factor)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = up_down(df, \"close\", 1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apply_technical_indicators(df, [\"open\", \"close\", \"high\", \"low\", \"volume\"], [7, 14, 20, 30, 50, 100])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_event_for_crossing(df, \"close_bollinger_lower_20\", \"close\")\n",
    "df = create_event_for_crossing(df, \"close\", \"close_bollinger_upper_20\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# df = percent_change(df, \"close\", [1, 7, 14, 30])\n",
    "# df = future_value(df, \"close\", 1)\n",
    "# df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = apply_event(df, dividends, prefix_name=\"dividends\", falloff=True, decay_factor=0.95)\n",
    "# df = apply_event(df, splits, prefix_name=\"splits\", falloff=True, decay_factor=0.95)\n",
    "# df = is_not_nan_column_and_default(df, [\"dividends_event_value\", \"dividends_event_sentiment\"])\n",
    "# df = is_not_nan_column_and_default(df, [\"splits_event_value\", \"splits_event_sentiment\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = create_lag_features(df, \"open\", lag_steps=list(range(1, 30)))\n",
    "df = create_lag_features(df, \"close\", lag_steps=list(range(1, 30)))\n",
    "df = create_lag_features(df, \"high\", lag_steps=list(range(1, 30)))\n",
    "df = create_lag_features(df, \"low\", lag_steps=list(range(1, 30)))\n",
    "df = create_lag_features(df, \"volume\", lag_steps=list(range(1, 30)))\n",
    "# df = create_lag_features(df, \"close_percent_change_1\", lag_steps=[1, 2, 3, 4, 5, 6, 7, 10, 14, 21, 30, 60])\n",
    "\n",
    "df = day_of_week(df)\n",
    "df = day_of_month(df)\n",
    "df = month(df)\n",
    "df = year(df)\n",
    "df = week_of_year(df)\n",
    "\n",
    "\n",
    "# df = create_lag_features(df, \"event_name\", lag_steps=list(range(1, 60)))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = macd(df, \"close\", 50, 200, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(like= \"open\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"../data/dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "#import xgbtune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/dataset.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = \"target_close\"\n",
    "target = \"close_percent_change_1\"\n",
    "# target = \"up_down_close_1\"\n",
    "\n",
    "x = df.drop(columns=[target]).set_index(\"date\")\n",
    "y = df[[\"date\", target]].set_index(\"date\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_info = pd.DataFrame()\n",
    "\n",
    "# Print columns that are not int, float, bool or category\n",
    "for col in x.columns:\n",
    "    dtype = x[col].dtype\n",
    "    if dtype not in [\"int\", \"float64\", \"bool\", \"category\"]:\n",
    "        table_info = pd.concat([table_info, pd.DataFrame({\"Column Name\": [col], \"Data Type\": [x[col].dtype]})])\n",
    "\n",
    "table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to float if they are not int, float, bool or category. Handle Cannot cast DatetimeArray to dtype float64 (XGBoosted models cannot use strings, but categories as enumerated values)\n",
    "for col in x.columns:\n",
    "    dtype = x[col].dtype\n",
    "    if dtype not in [\"int\", \"float64\", \"bool\", \"category\"]:\n",
    "        try:\n",
    "            x[col] = x[col].astype(\"float\")\n",
    "        except:\n",
    "            # drop datetime columns\n",
    "            x = x.drop(columns=[col])\n",
    "\n",
    "            print(f\"Dropped Column: {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalizing the features between 0 and 1\n",
    "# y_scaler = MinMaxScaler()\n",
    "# y = y_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# x_scaler = MinMaxScaler()\n",
    "# x = x_scaler.fit_transform(x)\n",
    "\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    All Features   | Target\n",
    "# +-----------------+---------+\n",
    "# | x_train         | y_train | <- 85% of the data which is used for training\n",
    "# |                 |         |\n",
    "# +-----------------+---------+\n",
    "# | x_test          | y_test  | <- 15% of the data which is used for testing\n",
    "# +-----------------+---------+\n",
    "\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, shuffle=False)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, shuffle=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True)\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from itertools import count, takewhile\n",
    "def frange(start, stop, step):\n",
    "    return takewhile(lambda x: x< stop, count(start, step))\n",
    "\n",
    "# list(frange(0, 1, 0.1))\n",
    "numpy.linspace(5, 10, num=6).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# model = xgb.XGBRegressor(n_estimators=100, max_depth=7, eta=0.1, subsample=1, colsample_bytree=.3)\n",
    "# model = xgb.XGBRegressor(n_estimators=150, max_depth=7, eta=0.05, subsample=1, colsample_bytree=.3)\n",
    "# model = xgb.XGBRegressor(n_estimators=100, max_depth=7, eta=0.05, subsample=1, colsampvle_bytree=.3)\n",
    "model = XGBClassifier(n_estimators=500, max_depth=5, eta=0.05, subsample=0.9, colsample_bytree=0.4)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "pickle.dump(model, open(\"../model/xgboost_model.pkl\", \"wb\")) # Save model as Python pickle object\n",
    "model.save_model(\"../model/xgboost_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.best_params_)\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(x_test)\n",
    "y_test[\"predicted\"] = predicted\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test[target], y_test[\"predicted\"])\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "balanced_accuracy_score = metrics.balanced_accuracy_score(y_test[target], y_test[\"predicted\"])\n",
    "print(\"balanced_accuracy_score:\", balanced_accuracy_score)\n",
    "\n",
    "recall = metrics.recall_score(y_test[target], y_test[\"predicted\"])\n",
    "print(\"recall:\", recall)\n",
    "\n",
    "precision = metrics.precision_score(y_test[target], y_test[\"predicted\"])\n",
    "print(\"precision:\", precision)\n",
    "\n",
    "average_precision = metrics.average_precision_score(y_test[target], y_test[\"predicted\"])\n",
    "print(\"average_precision:\", average_precision)\n",
    "\n",
    "f1 = metrics.f1_score(y_test[target], y_test[\"predicted\"])\n",
    "print(\"f1:\", f1)\n",
    "\n",
    "# more metrics: https://scikit-learn.org/stable/modules/model_evaluation.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy: 0.5153922542204568\n",
    "recall: 0.6316793893129771\n",
    "precision: 0.5287539936102237\n",
    "f1: 0.5756521739130435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidaence interval\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# configure bootstrap\n",
    "n_iterations = 1000\n",
    "n_size = int(len(y_test) * 0.5)\n",
    "# run bootstrap\n",
    "stats = list()\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # prepare train and test sets\n",
    "    test = resample(y_test, n_samples=n_size)\n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(test[target], test[\"predicted\"])\n",
    "    stats.append(accuracy)\n",
    "\n",
    "# plot scores\n",
    "plt.hist(stats)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test[target], y_test[\"predicted\"]))\n",
    "# print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test[target], y_test[\"predicted\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# indices\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(f\"{f + 1}. feature {x_train.columns[indices[f]]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted values\n",
    "y_test.sort_index().to_parquet(\"../data/predicted.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
